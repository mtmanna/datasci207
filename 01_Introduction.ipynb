{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtmanna/datasci207/blob/homework/01_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "# Lab 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "outputs": [],
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nNOD-Z7SzAq"
      },
      "source": [
        "## Data as matrices\n",
        "Data usually comes in the form of matrices. The Python Numpy library makes it easy to manipulate matrices efficiently. See the [Numpy Tutorial](https://docs.scipy.org/doc/numpy/user/quickstart.html) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWlmuAMwTZ3P"
      },
      "outputs": [],
      "source": [
        "# Print these to make sure you understand what is being generated.\n",
        "A = np.array([1, 2, 3])\n",
        "B = np.arange(1, 13).reshape(3, 4)\n",
        "C = np.ones((2, 3))\n",
        "D = np.eye(3)\n",
        "\n",
        "print(\"A:\\n\",A,\"\\nB:\\n\",B,\"\\nC:\\n\",C,\"\\nD:\\n\",D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4wvvzKoUIAN"
      },
      "source": [
        "---\n",
        "### Exercise 1: Matrix manipulation (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAOuoTD7_Vtm"
      },
      "source": [
        "Perform the following computations using numpy functions and print the results. Note that the `*` operator implies matrix multiplication -- make sure the dimensions align!\n",
        "1. 2A + 1\n",
        "2. Sum the rows of B\n",
        "3. Sum the columns of B\n",
        "4. Number of elements of B greater than 5\n",
        "5. C + C\n",
        "6. A * B\n",
        "7. (B * B) - D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJtwrjdO6TbS"
      },
      "outputs": [],
      "source": [
        "from numpy.core.numeric import count_nonzero\n",
        "# YOUR CODE HERE\n",
        "\n",
        "one = np.add(2 * A, 1)\n",
        "print(\"1.:\\n\",one,\"\\n\")\n",
        "\n",
        "two = np.sum(B,1)\n",
        "print(\"2.:\\n\",two,\"\\n\")\n",
        "\n",
        "three = np.sum(B,0)\n",
        "print(\"3.:\\n\",three,\"\\n\")\n",
        "\n",
        "four = count_nonzero(B>5)\n",
        "print(\"4.:\\n\",four,\"\\n\")\n",
        "\n",
        "five = np.add(C,C)\n",
        "print(\"5.:\\n\",five,\"\\n\")\n",
        "\n",
        "six = A * B.reshape(4,3)\n",
        "print(\"6.:\\n\",six,\"\\n\")\n",
        "\n",
        "extra_row_D = np.eye(3,4)\n",
        "seven = np.subtract((B * B), extra_row_D)\n",
        "print(\"7.:\\n\",seven)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHaXKwHf_Vtn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbCRG2-uUKCT"
      },
      "source": [
        "## Data for Supervised Learning\n",
        "Supervised learning is all about learning to make predictions: given an input $x$ (e.g. home square footage), can we produce an output $\\hat{y}$ (e.g. estimated value) as close to the actual observed output $y$ (e.g. sale price) as possible. Note that the \"hat\" above $y$ is used to denote an estimated or predicted value.\n",
        "\n",
        "Let's start by generating some artificial data. We'll create a vector of inputs, $X$, and a corresponding vector of target outputs $Y$. In general, we'll refer to invidual examples with a lowercase ($x$), and a vector or matrix containing multiple examples with a capital ($X$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ulmn_bFdU87t"
      },
      "outputs": [],
      "source": [
        "def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n",
        "  \"\"\"Create X, Y data with a linear relationship with added noise.\n",
        "\n",
        "  Args:\n",
        "    num_examples: number of examples to generate\n",
        "    w: desired slope\n",
        "    b: desired intercept\n",
        "    random_scale: add uniform noise between -random_scale and +random_scale\n",
        "\n",
        "  Returns:\n",
        "    X and Y with shape (num_examples)\n",
        "  \"\"\"\n",
        "  X = np.arange(num_examples)\n",
        "  np.random.seed(4)  # consistent random number generation\n",
        "  deltas = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n",
        "  Y = b + deltas + w * X\n",
        "  return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qJg0IiYVJ8U"
      },
      "outputs": [],
      "source": [
        "# Create some artificial data using create_1d_data.\n",
        "X, Y = create_1d_data(18,18.3,0,5)\n",
        "plt.scatter(X, Y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6coKbXSpXOz"
      },
      "source": [
        "---\n",
        "### Exercise 2: Models for Data (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8wftA67_Vto"
      },
      "source": [
        "A model is a function that takes an input $x$ and produces a prediction $\\hat{y}$.\n",
        "\n",
        "Let's consider two possible models for this data:\n",
        "1. $M_1(x) = x+5$ \n",
        "2. $M_2(x) = 2x+1$\n",
        "\n",
        "Compute the predictions of models $M_1$ and $M_2$ for the values in $X$. These predictions should be vectors of the same shape as $Y$. Then plot the prediction lines of these two models overlayed on the \"observed\" data $(X, Y)$. Use [plt.plot()](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html) to draw the lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHIY5kNXUIAP"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "M1 = X + 5\n",
        "M2 = 2 * X + 1\n",
        "plt.plot(M1)\n",
        "plt.plot(M2)\n",
        "plt.scatter(X,Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfCGTUT0_Vtp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH-0soZiWx9x"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "How good are our models? Intuitively, the better the model, the more closely it fits the data we have. That is, for each $x$, we'll compare $y$, the true value, with $\\hat{y}$, the predicted value. This comparison is often called the *loss* or the *error*. One common such comparison is *squared error*: $(y-\\hat{y})^2$. Averaging over all our data points, we get the *mean squared error*:\n",
        "\n",
        "\\begin{equation}\n",
        "\\textit{MSE} = \\frac{1}{|Y|} \\sum_{y_i \\in Y}(y_i - \\hat{y}_i)^2\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AyY2DpxYLI0"
      },
      "source": [
        "---\n",
        "### Exercise 3: Computing MSE (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDU82NXI_Vtq"
      },
      "source": [
        "Write a function for computing the MSE metric and use it to compute the MSE for the two models above, $M_1$ and $M_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCeAfI5mW9sg"
      },
      "outputs": [],
      "source": [
        "def MSE(true_values, predicted_values):\n",
        "  \"\"\"Return the MSE between true_values and predicted values.\n",
        "   \n",
        "  Args:\n",
        "    true_values: data points\n",
        "    predicted_values: values from prediction models\n",
        "\n",
        "  Returns:\n",
        "    Mean Squared Error\n",
        "  \"\"\"\n",
        "\n",
        "  total = count_nonzero(true_values)\n",
        "  loss = 1/total * sum((true_values - predicted_values)**2)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF-x9DI2ZOKq"
      },
      "outputs": [],
      "source": [
        "print ('MSE for M1:', MSE(Y, M1))\n",
        "print ('MSE for M2:', MSE(Y, M2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUaC9dMy_Vtq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDiy3OZwZlwj"
      },
      "source": [
        "## Generalization\n",
        "\n",
        "Our data $(X, Y)$ represents just a sample of all possible input-output pairs we might care about. A model will be useful to the extent we can apply it to new inputs. Consider the more complex model below, which appears to produce a much smaller mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns1siZ9DZvSY"
      },
      "outputs": [],
      "source": [
        "# Fit an 8-th degree polynomial to (X, Y). See np.polyfit for details.\n",
        "polynomial_model_coefficients = np.polyfit(X, Y, deg=8)\n",
        "polynomial_model = np.poly1d(polynomial_model_coefficients)\n",
        "M3 = polynomial_model(X)\n",
        "fig = plt.scatter(X, Y)\n",
        "plt.plot(X, M3, '-k')\n",
        "print ('MSE for M3:', MSE(Y, M3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2m9YmLMZ1EV"
      },
      "source": [
        "---\n",
        "### Exercise 4: Generalization (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7xgVuOQ_Vtr"
      },
      "source": [
        "Explain whether you expect $M_3$ to be better than $M_2$ at predicting the labels for new unseen inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Zpx79_aQEC"
      },
      "source": [
        "*Writen answer:*\n",
        "Yes, I would expect M3 to be better than M2 at predicting the labels for new unseen inputs. M2 is simplistic and is essentially a shot in the dark for the slope and intercept of the trend line of the data. It cannot adapt or learn from test data. M3 utilizes the test data and fits the line much better. The MSE is significantly lower than M2. However, there also runs the risk of overfitting the model, so while M3 works well for the current test inputs, we need to be cautious that it may fit to our test data too specifically, which could decrease its liklihood of consistently predicting the values for new inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9EH9D7Faf9n"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hIdZHngdrET"
      },
      "source": [
        "## Review\n",
        "\n",
        "* In **Supervised Machine Learning**, we must start with data in the form $(X,Y)$ where $X$ are the inputs and $Y$ are the output labels.\n",
        "* A **model** is a function that maps an input $x$ to an output $y$. The model's output is referred to as a **prediction**, denoted by $\\hat{y}$.\n",
        "* We **evaluate** predictions by comparing them to the true labels. This measurement is called a **loss** or **error**. For real-valued data, **mean squared error** is a common metric.\n",
        "* A model is only as good as its ability to **generalize** to new examples."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}